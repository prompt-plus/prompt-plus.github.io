<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>prompt+</title>
<link href="style.css" rel="stylesheet">
</head>

<body>
<div class="content">
  <h1>P+: Extended Textual Conditioning in Text-to-Image Generation</h1>
  <p id="authors"><a href="https://scholar.google.com/citations?user=imBjSgUAAAAJ">Andrey Voynov<sup>1</sup></a> <a>Qinghao Chu<sup>1</sup></a> <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or<sup>1,2</sup></a> <a href="https://kfiraberman.github.io/">Kfir Aberman<sup>1</sup></a><br>

  <span style="font-size: 16px"><sup>1</sup> Google Research &nbsp;&nbsp;<sup>2</sup> Tel Aviv University
  </span></p>
  <br><img src="./files/mix_teaser.jpeg" class="teaser-img" style="width:60%;"><br>

  <font size="+2">
    <p style="text-align: center;">
      <a href="./files/PromptPlus.pdf" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
    </p>
  </font>
</div>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>We introduce an Extended Textual Conditioning space in text-to-image models, referred to as P+. This space consists of multiple textual conditions, derived from per-layer prompts, each corresponding to a layer of the denoising U-net of the diffusion model. 
    We show that the extended space provides greater disentangling and control over image synthesis. We further introduce Extended Textual Inversion (XTI), where the images are inverted into P+, and represented by per-layer tokens.
    We show that XTI
    is more expressive and precise, and converges faster than the original Textual Inversion (TI) space.
    The extended inversion method does not involve any noticeable trade-off between reconstruction and editability and induces more regular inversions.
    We conduct a series of extensive experiments to analyze and understand the properties of the new space, and to showcase the effectiveness of our method for personalizing text-to-image models. Furthermore, we utilize the unique properties of this space to achieve previously unattainable results in object-style mixing using text-to-image models.
  </p>

  <div>
    <img class="summary-img" src="./files/p_pp_sbs.jpeg" style="width:50%;">
    <i>Standard textual conditioning, where a single text embedding is injected to the network (left), vs. our proposed extended conditioning, where different embeddings are injected into different layers of the U-net (right).</i>
  </div> 
  <br>
</div>

<div class="content">
    <h2>Improved Concept Inversion</h2>
    <div>
        <img class="summary-img" src="./files/inversion_examples.jpeg" style="width:90%;">
    </div> 
    <p>
    Textual Inversion (TI) vs. Extended Textual Inversion (XTI).
    Column 1: Original concepts. Column 2: TI results. Column 3: XTI results. It can be seen that XTI exhibits superior subject and prompt fidelity, as corroborated by the results of our user study.
    </p>
    <br>
</div>

<div class="content">
    <h2>Style Mixing</h2>
    We can mix geometry and appearance by passing different subjects inversions to coarse and fine layers of the diffusion denoising U-net.
    <div>
        <img class="summary-img" src="./files/mix_samples.jpeg" style="width:90%;">
    </div> 
    <i> Geometry-style mixing with a shape taken from first column, and style from top row</i>
    <br>
</div>